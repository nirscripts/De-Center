{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12de462a",
   "metadata": {},
   "source": [
    "# De-Center: Exploratory Data Analysis (EDA)\n",
    "## Analyzing Policy Claims vs. Community Impact Contradictions\n",
    "\n",
    "This notebook explores the contradiction dataset collected from official policy documents, news articles, and local filings regarding California's AI data center environmental and water impacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5130a55d",
   "metadata": {},
   "source": [
    "## 1. Load and Inspect the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f6ab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Load ground truth contradictions\n",
    "with open('../data/ground_truth.json', 'r') as f:\n",
    "    ground_truth = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(ground_truth)} contradiction pairs\\n\")\n",
    "print(\"Sample contradiction:\")\n",
    "print(json.dumps(ground_truth[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d4987e",
   "metadata": {},
   "source": [
    "## 2. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f4a180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for easier analysis\n",
    "df = pd.DataFrame(ground_truth)\n",
    "\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Total contradictions: {len(df)}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7602c083",
   "metadata": {},
   "source": [
    "## 3. Analyze Contradiction Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16435f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Issue distribution\n",
    "issue_counts = df['issue'].value_counts()\n",
    "print(\"=== CONTRADICTIONS BY ISSUE ===\")\n",
    "print(issue_counts)\n",
    "print(f\"\\nTotal unique issues: {len(issue_counts)}\")\n",
    "\n",
    "# Visualize issue distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "issue_counts.plot(kind='barh', ax=ax, color='steelblue')\n",
    "ax.set_xlabel('Number of Contradictions')\n",
    "ax.set_ylabel('Issue Type')\n",
    "ax.set_title('Contradiction Distribution by Issue')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../data/processed/issue_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\n✓ Chart saved to data/processed/issue_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c6096b",
   "metadata": {},
   "source": [
    "## 4. Analyze Source Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c216e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract source types from URLs\n",
    "source_a_counts = Counter()\n",
    "source_b_counts = Counter()\n",
    "\n",
    "def categorize_source(url):\n",
    "    \"\"\"Categorize source by domain/type\"\"\"\n",
    "    if 'calmatters' in url:\n",
    "        return 'News (CalMatters)'\n",
    "    elif 'gov.ca.gov' in url or 'cpuc' in url:\n",
    "        return 'Government (CA)'\n",
    "    elif 'next10.org' in url:\n",
    "        return 'Research (Next10/UCR)'\n",
    "    elif 'legistar' in url:\n",
    "        return 'Government (Local)'\n",
    "    elif 'siliconvalleypower' in url:\n",
    "        return 'Utility (SVP)'\n",
    "    elif 'sanjose' in url:\n",
    "        return 'Government (San Jose)'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "for row in ground_truth:\n",
    "    source_a_counts[categorize_source(row['source_a'])] += 1\n",
    "    source_b_counts[categorize_source(row['source_b'])] += 1\n",
    "\n",
    "print(\"=== SOURCE A (Official Claims) ===\")\n",
    "for source, count in source_a_counts.most_common():\n",
    "    print(f\"  {source}: {count}\")\n",
    "\n",
    "print(\"\\n=== SOURCE B (Community Impact Data) ===\")\n",
    "for source, count in source_b_counts.most_common():\n",
    "    print(f\"  {source}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad943743",
   "metadata": {},
   "source": [
    "## 5. Contradiction Claims Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31918f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze claim lengths and keywords\n",
    "print(\"=== OFFICIAL CLAIM ANALYSIS ===\")\n",
    "official_lengths = [len(row['official_claim'].split()) for row in ground_truth]\n",
    "print(f\"Average words per official claim: {sum(official_lengths) / len(official_lengths):.1f}\")\n",
    "print(f\"Min/Max: {min(official_lengths)}/{max(official_lengths)} words\")\n",
    "\n",
    "print(\"\\n=== CONTRADICTORY DATA ANALYSIS ===\")\n",
    "contrary_lengths = [len(row['contradictory_data'].split()) for row in ground_truth]\n",
    "print(f\"Average words per contradictory data: {sum(contrary_lengths) / len(contrary_lengths):.1f}\")\n",
    "print(f\"Min/Max: {min(contrary_lengths)}/{max(contrary_lengths)} words\")\n",
    "\n",
    "# Show sample claims\n",
    "print(\"\\n=== SAMPLE CLAIMS ===\")\n",
    "for i, row in enumerate(ground_truth[:1], 1):\n",
    "    print(f\"\\nContradiction {i}: {row['issue']}\")\n",
    "    print(f\"Official: {row['official_claim'][:100]}...\")\n",
    "    print(f\"Counter-data: {row['contradictory_data'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab463f7",
   "metadata": {},
   "source": [
    "## 6. Key Metrics for Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d091607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics for later model evaluation\n",
    "print(\"=== BENCHMARK METRICS ===\")\n",
    "print(f\"Total Contradictions (Ground Truth): {len(ground_truth)}\")\n",
    "print(f\"\\nMetrics for model evaluation:\")\n",
    "print(f\"  - Recall: (Detected Contradictions) / (Actual {len(ground_truth)})\")\n",
    "print(f\"  - Precision: (True Contradictions) / (Total Detected)\")\n",
    "print(f\"  - F1-Score: 2 * (Precision * Recall) / (Precision + Recall)\")\n",
    "print(f\"\\nBaseline (random):\")\n",
    "print(f\"  - Expected Recall: 0.5\")\n",
    "print(f\"  - Expected Precision: ~(Contradictions / All Pairs)\")\n",
    "\n",
    "# Calculate data coverage\n",
    "print(f\"\\n=== DATA COVERAGE ===\")\n",
    "print(f\"Topics covered: water usage, oversight delays, emissions reporting\")\n",
    "print(f\"Geographic scope: California (state-level policy + local impacts)\")\n",
    "print(f\"Timeframe: 2019-2028 (historical + projected)\")\n",
    "print(f\"Source diversity: {len(set([row['source_a'] for row in ground_truth] + [row['source_b'] for row in ground_truth]))} unique sources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212b6fba",
   "metadata": {},
   "source": [
    "## 7. Data Gaps and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8951bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== IDENTIFIED GAPS ===\")\n",
    "print(\"1. Local filing evidence: Limited 2026 San Jose/Santa Clara rate-hike or water-restriction language\")\n",
    "print(\"2. Scope of contradictions: Currently 3 pairs - need 10-20 for robust model training\")\n",
    "print(\"3. Annotation quality: Needs multi-annotator review and uncertainty scoring\")\n",
    "print(f\"\\n=== NEXT STEPS ===\")\n",
    "print(\"1. Run web scraper to expand ground_truth.json with additional contradictions\")\n",
    "print(\"2. Add severity and confidence scores to each contradiction pair\")\n",
    "print(\"3. Create training/test splits for model evaluation\")\n",
    "print(\"4. Develop contradiction detection baseline (keyword matching)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15770c83",
   "metadata": {},
   "source": [
    "## 8. Export Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431a0e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "summary_report = {\n",
    "    'dataset_name': 'De-Center: California Data Center Policy Contradictions',\n",
    "    'total_contradictions': len(ground_truth),\n",
    "    'unique_issues': list(df['issue'].unique()),\n",
    "    'data_collection_date': '2026-02-21',\n",
    "    'key_statistics': {\n",
    "        'avg_official_claim_length': round(sum(official_lengths) / len(official_lengths), 1),\n",
    "        'avg_contrary_data_length': round(sum(contrary_lengths) / len(contrary_lengths), 1),\n",
    "        'unique_sources': len(set([row['source_a'] for row in ground_truth] + [row['source_b'] for row in ground_truth]))\n",
    "    },\n",
    "    'source_distribution': {\n",
    "        'source_a': dict(source_a_counts),\n",
    "        'source_b': dict(source_b_counts)\n",
    "    },\n",
    "    'next_steps': [\n",
    "        'Expand dataset via web scraper',\n",
    "        'Add severity and confidence scores',\n",
    "        'Create train/test splits',\n",
    "        'Build contradiction detection baseline'\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('../data/processed/eda_summary.json', 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2)\n",
    "\n",
    "print(\"✓ EDA summary saved to data/processed/eda_summary.json\")\n",
    "print(json.dumps(summary_report, indent=2))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
